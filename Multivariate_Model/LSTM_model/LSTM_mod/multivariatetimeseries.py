# -*- coding: utf-8 -*-
"""multiVariateTimeSeries.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/199XdwIfaayD0wnn0Xrm7lOMi6KWp2A9t

**Multivariate Time Series Forecasting on temperature data**
"""

import tensorflow as tf
import os
import pandas as pd
import numpy as np

zip_path = tf.keras.utils.get_file(
    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',
    fname='jena_climate_2009_2016.csv.zip',
    extract=True)
csv_path, _ = os.path.splitext(zip_path)

data = pd.read_csv(csv_path)
data

# Want to gt every hour
data = data[5::6]
data

# Set date to datetime
data['Date Time'] = pd.to_datetime(data['Date Time'], format='%d.%m.%Y %H:%M:%S')
data.set_index(data['Date Time'], inplace=True)
data

temp_data = data['T (degC)']
temp_data

temp_data.plot(subplots=True)

temp_data.isna().sum()

# Want to divide the time series into a machine learning type of objective
def df_to_X_y(df, window_size=5):
  df_as_np = df.to_numpy()
  X = []
  y = []
  for i in range(len(df_as_np)-window_size):
    row = [[a] for a in df_as_np[i:i+window_size]]
    X.append(row)
    label = df_as_np[i+window_size]
    y.append(label)
  return np.array(X), np.array(y)

# Univariate Case
X, y = df_to_X_y(temp_data, window_size=5)
X.shape, y.shape

# Split the data into train, val, and test set
X_train , y_train = X[:60000], y[:60000]
X_val, y_val = X[60000:65000], y[60000:65000]
X_test, y_test = X[65000:], y[65000:]

X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape

# Define the model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, InputLayer
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError


model = Sequential()
model.add(InputLayer(input_shape=(5, 1)))
model.add(LSTM(64))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='linear'))

model.compile(loss=MeanSquaredError(),
              optimizer=Adam(learning_rate=0.0001),
              metrics=[RootMeanSquaredError()])

model.summary()

# Time to train the model
checkpoint = ModelCheckpoint('model1/model_checkpoint.keras', save_best_only=True)
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=40,
                    batch_size=256,
                    callbacks=[checkpoint])

from tensorflow.keras.models import load_model
model = load_model('model1/model_checkpoint.keras')

train_pred = model.predict(X_train).flatten()
train_res = pd.DataFrame({'Actual': y_train, 'Predicted': train_pred})
train_res

# Plot the results
from matplotlib import pyplot as plt
plt.plot(train_res['Actual'][:100])
plt.plot(train_res['Predicted'][:100])

# Multivarite Case starts here!
# Start by incorporating seasonality by using sequence-based functions

# Step 1: get amount of secons from datetime
temp_df = pd.DataFrame(data={'Temp': temp_data})
temp_df['Seconds'] = temp_df.index.map(pd.Timestamp.timestamp)
temp_df

day_to_seconds = 24*60*60
year_to_seconds = 365*day_to_seconds

temp_df['Day sin'] = np.sin(temp_df['Seconds'] * (2 * np.pi / day_to_seconds))
temp_df['Day cos'] = np.cos(temp_df['Seconds'] * (2 * np.pi / day_to_seconds))
temp_df['Year sin'] = np.sin(temp_df['Seconds'] * (2*np.pi / year_to_seconds))
temp_df['Year cos'] = np.cos(temp_df['Seconds'] * (2*np.pi / year_to_seconds))

temp_df

# Lets drop the seconds column now
temp_df.drop('Seconds', inplace=True, axis=1)
temp_df

# Now its time to get the matrices for training
def df_to_X_y_2(df, window_size=6):
  df_as_np = df.to_numpy()
  X = []
  y = []
  for i in range(len(df_as_np)-window_size):
    row = [a for a in df_as_np[i:i+window_size]]
    X.append(row)
    label = df_as_np[i+window_size][0]
    y.append(label)
  return np.array(X), np.array(y)

X, y = df_to_X_y_2(temp_df)
X.shape, y.shape

X2_train, y2_train = X[:60000], y[:60000]
X2_val, y2_val = X[60000:65000], y[60000:65000]
X2_test, y2_test = X[65000:], y[65000:]

X2_train.shape, y2_train.shape, X2_val.shape, y2_val.shape, X2_test.shape, y2_test.shape

# Need to preprocess X for scaling purposes
def preprocess_X(X):
  X[:, :, 0] = (X[:, :, 0] - X[:, :, 0].mean()) / X[:, :, 0].std()
  return X

X2_train = preprocess_X(X2_train)
X2_val = preprocess_X(X2_val)
X2_test = preprocess_X(X2_test)

# Time to build our model
from keras.regularizers import l2

model3 = Sequential()
model3.add(InputLayer(input_shape=(6, 5)))
model3.add(LSTM(64, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))
model3.add(Dense(8, activation='relu'))
model3.add(Dense(1, activation='linear'))

model3.compile(loss=MeanSquaredError(),
              optimizer=Adam(learning_rate=0.0001),
              metrics=[RootMeanSquaredError()])

model3.summary()

cp3 = ModelCheckpoint('model3/model_checkpoint.keras', save_best_only=True)

model3.fit(X2_train, y2_train,
           validation_data=(X2_val, y2_val),
           epochs=20,
           callbacks=[cp3])

test_pred = model3.predict(X2_test).flatten()
test_res = pd.DataFrame({'Actual': y2_test, 'Predicted': test_pred})
test_res

plt.plot(test_res['Actual'][:100])
plt.plot(test_res['Predicted'][:100])